---
title: "Peer Assessment I"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---


First, let us load the data and necessary packages:

```{r load, message = FALSE}
load("ames_train.Rdata")
library(MASS)
library(dplyr)
library(ggplot2)
```

#
Make a labeled histogram (with 30 bins) of the ages of the houses in the data set, and describe the distribution.


```{r Q1}
ames_train %>% summarise(min = min(Year.Built), max = max(Year.Built), mean = mean(Year.Built), median = median(Year.Built), sd = sd(Year.Built)) #102 distinct values
2010-1872
# Create a histogram with 30 bins of Year.Built
ggplot(data = ames_train, aes(Year.Built)) + geom_histogram(binwidth = (140/30))


```

* * *
The data range from 1972 to 2010, appear to be left skewed, and are multi-modal. The mean is approximately 1972, the median is 1975, and the standard deviation is approximately 30 years.

* * *


#
The mantra in real estate is "Location, Location, Location!" Make a graphical display that relates a home price to its neighborhood in Ames, Iowa. Which summary statistics are most appropriate to use for determining the most expensive, least expensive, and most heterogeneous (having the most variation in housing price) neighborhoods? Report which neighborhoods these are based on the summary statistics of your choice. Report the value of your chosen summary statistics for these neighborhoods.


```{r Q2}
ggplot(data = ames_train, aes(Neighborhood, price)) + geom_boxplot() + coord_flip()
summary_stats <- 
  ames_train %>% 
  filter(!is.na(price)) %>%
  group_by(Neighborhood) %>% 
  summarise(min = min(price), max = max(price), mean = mean(price), median = median(price), sd = sd(price))
```


* * *

The Oldtown Neighborhood has the lowest priced home, at \$12,789, and the fourth lowest mean (\$120,225) and median (\$120,000) values for all homes in the neighbordhood.
Meadow Valley has the lowest mean (\$92,946.88) and median (\$85,750) home values, and is fairly homogenous by home value.
The highest priced home is in Northridge Heights, at \$615,000. Northridge Heights also had the second highest mean and median values, and is fairly heterogenous, but isn't the most heterogenous.
The neighborhood with the most price diversity was Stonebridge, with a standard deviation of \$123,459. Stonebridge also has the highest mean and median home prices.


* * *

# 

Which variable has the largest number of missing values? Explain why it makes sense that there are so many missing values for this variable.

```{r Q3}
missing <- sapply(ames_train, function(x) sum(is.na(x)))
missing[missing>0]
```


* * *

Pool.QC seems to have the most missing data (997 cases), which makes sense, as this value would only be filled if the property has a pool. Misc.Feature, Alley, Fence, and Fireplace.Qu also have a large number of missing values, which is also expected as those fields represent non-required features of a property, where missing values indicates an absence of those features.


* * *

#

We want to predict the natural log of the home prices. Candidate explanatory variables are lot size in square feet (Lot.Area), slope of property (Land.Slope), original construction date (Year.Built), remodel date (Year.Remod.Add), and the number of bedrooms above grade (Bedroom.AbvGr). Pick a model selection or model averaging method covered in the Specialization, and describe how this method works. Then, use this method to find the best multiple regression model for predicting the natural log of the home prices.


```{r Q4}
model_base <- lm(data = ames_train, log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)

model2 <- model2 <- lm(data = ames_train, log(price) ~ Lot.Area + Year.Built + Year.Remod.Add + Bedroom.AbvGr)
model3 <- model2 <- lm(data = ames_train, log(price) ~ Lot.Area + Land.Slope + Year.Remod.Add + Bedroom.AbvGr)
model4 <- model2 <- lm(data = ames_train, log(price) ~ Lot.Area + Land.Slope + Year.Built + Bedroom.AbvGr)
model5 <- model2 <- lm(data = ames_train, log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add)
model6 <- lm(data = ames_train, log(price) ~ Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)
model_results <- data.frame(c("model_base", "model2", "model3", "model4", "model5", "model6"), c(summary(model_base)$adj.r.squared, summary(model2)$adj.r.squared, summary(model3)$adj.r.squared, summary(model4)$adj.r.squared, summary(model5)$adj.r.squared, summary(model6)$adj.r.squared))
names(model_results) <- c("model", "adj.r.squared")
model_results
summary(model_base)
```

* * *

We use a backwards step-wise selection approach using adjusted r^2 values to select a multiple linear regression. The base model, which includes all variables, It shows that all variables are significant, and contribute to a higher adjusted R^2 value. The second step, where variables are individually removed from the regression equation, shows no increase in adjusted r^2, indicating that the base model is the best model for predicting the natural log of home prices.


* * *

#

Which home has the largest squared residual in the previous analysis (Question 4)? Looking at all the variables in the data set, can you explain why this home stands out from the rest (what factors contribute to the high squared residual and why are those factors relevant)?


```{r Q5}
comparison <- data.frame(model_base$fitted.values, ames_train$price)
comparison <- comparison %>% mutate(sq.residual = model_base.fitted.values - log(ames_train.price))
comparison %>% filter(sq.residual == max(sq.residual))
ames_train %>% filter(price == 12789)
plot(model_base$residuals)
```

* * *

The value with the highest squared residual appears to be the lowest priced home in the sample, a 2 bedroom, 1 bathroom house in the Old Town neighborhood. The "abnormal" sale condition (which typically indicates a distressed sale condition, such as a foreclosure) may be a factor in the low price. The home is also of "poor" overall quality and condition.


* * *

#

Use the same model selection method you chose in Question 4 to again find the best multiple regression model to predict the natural log of home prices, but this time **replacing Lot.Area with log(Lot.Area)**. Do you arrive at a model including the same set of predictors?


```{r Q6}
model_base <- lm(data = ames_train, log(price) ~ log(Lot.Area) + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)


model2 <- model2 <- lm(data = ames_train, log(price) ~ log(Lot.Area) + Year.Built + Year.Remod.Add + Bedroom.AbvGr)
model3 <- model2 <- lm(data = ames_train, log(price) ~ log(Lot.Area) + Land.Slope + Year.Remod.Add + Bedroom.AbvGr)
model4 <- model2 <- lm(data = ames_train, log(price) ~ log(Lot.Area) + Land.Slope + Year.Built + Bedroom.AbvGr)
model5 <- model2 <- lm(data = ames_train, log(price) ~ log(Lot.Area) + Land.Slope + Year.Built + Year.Remod.Add)
model6 <- lm(data = ames_train, log(price) ~ Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)
model_results <- data.frame(c("model_base", "model2", "model3", "model4", "model5", "model6"), c(summary(model_base)$adj.r.squared, summary(model2)$adj.r.squared, summary(model3)$adj.r.squared, summary(model4)$adj.r.squared, summary(model5)$adj.r.squared, summary(model6)$adj.r.squared))
names(model_results) <- c("model", "adj.r.squared")
model_results
summary(model_base)

```

* * *

When we re-select the model using the same methods, but this time using the natural log of lot area, the best fit model is still the base model (including all candidate variables).
#

Do you think it is better to log transform Lot.Area, in terms of assumptions for linear regression? Make graphs of the predicted values of log home price versus the true values of log home price for the regression models selected for Lot.Area and log(Lot.Area). Referencing these two plots, provide a written support that includes a quantitative justification for your answer in the first part of question 7.

```{r Q7}
model_base <- lm(data = ames_train, log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)
model_base_log <- lm(data = ames_train, log(price) ~ log(Lot.Area) + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr)
plot(log(ames_train$price), col = "black", type = "line")
plot(model_base$residuals, col = "green")
points(model_base_log$residuals, col = "blue")



```

* * *

NOTE: Write your written response to Question 7 here.  Delete this note before you submit your work.


* * *
###