---
title: "Peer Assessment II"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---

# Background

As a statistical consultant working for a real estate investment firm, your task is to develop a model to predict the selling price of a given home in Ames, Iowa. Your employer hopes to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.

# Training Data and relevant packages

In order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.

```{r load, message = FALSE}
load("ames_train.Rdata")
```

Use the code block below to load any necessary packages

```{r packages, message = FALSE}
library(statsr)
library(dplyr)
library(BAS)
library(ggplot2)
library(scales)
library(GGally)
library(MASS)
```

## Part 1 - Exploratory Data Analysis (EDA)

When you first get your data, it's very tempting to immediately begin fitting models and assessing how they perform.  However, before you begin modeling, it's absolutely essential to explore the structure of the data and the relationships between the variables in the data set.

Do a detailed EDA of the ames_train data set, to learn about the structure of the data and the relationships between the variables in the data set (refer to Introduction to Probability and Data, Week 2, for a reminder about EDA if needed). Your EDA should involve creating and reviewing many plots/graphs and considering the patterns and relationships you see. 

After you have explored completely, submit the three graphs/plots that you found most informative during your EDA process, and briefly explain what you learned from each (why you found each informative).

* * *

The three datasets used in this analysis come from the AmesHousing data, which, according to the codebook documentation, "contains information from the Ames [Iowa] Assessor's Office ... from 2006 to 2010." The total dataset includes 2930 observations. These data have been divided into training, test, and validation datasets for use in this analysis. It's worth noting that these observations were taken from a time period that included the end of a period of record home prices, and the start of the so-called Great Recession, during which housing prices were severely depressed due to the sub-prime mortgage crisis. As such, modeled results may imperfectly reflect the current market conditions.

The training data include 1000 observations of 81 data points. 

Most fields are complete, where each of the 1000 observations has a value, but there are some fields missing data. All fields missing data are from "optional" fields, in that if a property does not contain a feature, the field will not be filled. For example, there are 21 properties that do not have basements, so the basement quality field for those rows is not filled. In these cases, lack of a feature is of interest, as the absence of a basement or a pool might be a significant predictor of sale price. I've created "dummy variables" to note properties that do not have certain features. These variables display a TRUE value if a property has a feature, and a FALSE value if not. Once the dummy variables are created, the more detailed data are dropped from the dataset.

There are several observations where a sale was made under abnormal cicumstances. We only want to model the price for properties that were sold under normal conditions, as abnormal sales conditions are often under unique circumstances that lead to a differences in the final purchase price that are otherwise very difficult to predict and are not useful for our uses. There are 166 records that sold for non-normal circumstances. These will be removed from the training data, leaving us with 834 records. The Sale.Condition field will then be dropped from the dataset.

```{r behind_the_scenes, message=FALSE}
# Show fields missing values:
missing <- sapply(ames_train, function(x) sum(is.na(x)))

# Identidy and remove abnormal sale conditions:
ames_train <- ames_train %>% filter(Sale.Condition == "Normal")
ames_train <- ames_train[!names(ames_train) == "Sale.Condition"]

# Convert integer MS.SubClass to data type character because it's a discrete variable loaded as a continuous variable.
ames_train$MS.SubClass <- as.character(ames_train$MS.SubClass)
ames_train$MS.SubClass[nchar(ames_train$MS.SubClass)==2] <- paste0("0",ames_train$MS.SubClass[nchar(ames_train$MS.SubClass)==2])

# Convert NA Alley values to "No alley," to make the values usable in the model.
ames_train$Alley_vals <- as.character(ames_train$Alley)
ames_train$Alley_vals[is.na(ames_train$Alley_vals)] <- "No alley"

# Because not all houses have basements, it's not advisable to use basement quality as an independent variable. This
# replaces NA Bsmt.Qual values with FALSE, indicating that house does not have a basement, and all other values with TRUE,
# indicating those houses do have a basement.
ames_train$basement_dummy <- TRUE
ames_train$basement_dummy[is.na(ames_train$Bsmt.Qual) | ames_train$Bsmt.Qual == ""] <- FALSE

# Do the same with garage
ames_train$garage_dummy <- TRUE
ames_train$garage_dummy[is.na(ames_train$Garage.Type)] <- FALSE

# Do the same with deck
ames_train$deck_dummy <- FALSE
ames_train$deck_dummy[ames_train$Wood.Deck.SF>0] <- TRUE

# do the same with the various porch types
ames_train$porch_dummy <- FALSE
ames_train$porch_dummy[ames_train$Open.Porch.SF>0] <- TRUE
ames_train$porch_dummy[ames_train$Enclosed.Porch>0] <- TRUE
ames_train$porch_dummy[ames_train$X3Ssn.Porch>0] <- TRUE
ames_train$porch_dummy[ames_train$Screen.Porch>0] <- TRUE

# Do the same with pool
ames_train$pool_dummy <- FALSE
ames_train$pool_dummy[ames_train$Pool.Area>0] <- TRUE

# do the same with Fence
ames_train$fence_dummy <- TRUE
ames_train$fence_dummy[is.na(ames_train$Fence)] <- FALSE

# I want to consider the number of total bathrooms as a single variable. This converts the two separate bathroom variables
# to a single variable by multiplying the number of full bathrooms by 1 and adding that to the number of half bathrooms multiplied by 1/2
ames_train <- ames_train %>% mutate(bathrooms = Full.Bath * 1 + Half.Bath * 0.5)

# As discussed above, we want to examine area and lot.area as logged values, as they are more linearly related to log price in that way.
ames_train$area <- log(ames_train$area)
ames_train$Lot.Area <- log(ames_train$Lot.Area)

#Create a matrix of Neighborhood
ames_train <- cbind(ames_train, model.matrix(~Neighborhood, ames_train))
ames_train <- cbind(ames_train,model.matrix(~MS.SubClass, ames_train))
ames_train <- cbind(ames_train,model.matrix(~House.Style, ames_train))
ames_train <- cbind(ames_train,model.matrix(~Roof.Style, ames_train))
ames_train <- cbind(ames_train,model.matrix(~Roof.Matl, ames_train))

# The following drops all variables that will not be considered in the full model.
drops <- c("PID", "Neighborhood", "Lot.Frontage", "Bsmt.Qual", "Bsmt.Cond", "Bsmt.Exposure", "BsmtFin.Type.1", "BsmtFin.Type.2", "BsmtFin.SF.1", "BsmtFin.SF.2", "Bsmt.Unf.SF", 
           "Total.Bsmt.SF", "Bsmt.Full.Bath", "Bsmt.Half.Bath", "Garage.Type", "Garage.Yr.Blt", "Garage.Finish", "Garage.Cars", 
           "Garage.Area", "Garage.Qual", "Garage.Cond", "Wood.Deck.SF", "Open.Porch.SF", "Enclosed.Porch", "X3Ssn.Porch",
           "Screen.Porch", "Pool.Area", "Fence", "Fireplace.Qu", "Pool.QC", "Misc.Feature", "Misc.Val", "Yr.Sold", "Mo.Sold", "Sale.Type", 
           "Sale.Condition", "Mas.Vnr.Type", "Mas.Vnr.Area", "X1st.Flr.SF", "X2nd.Flr.SF", "Low.Qual.Fin.SF", "TotRms.AbvGrd", "Condition.2","Alley", "Utilities", "Exter.Cond",
           "House.Style", "Roof.Style", "Roof.Matl", "Exterior.1st", "Exterior.2nd", "Foundation", "MS.SubClass", "(Intercept)", "(Intercept).1", "(Intercept).2", "(Intercept).3",
           "(Intercept).4", "(Intercept).5")
ames_train_red <- ames_train[,!(names(ames_train) %in% drops)]

```


Our dependent variable in the upcoming analysis is home price. Price is right skewed, as you might expect with a continuous variable bound by $0 on the low and hypothetically unbound on the high. The median is \$159,467. Logging the data gives us a nearly normal distribution, as shown in the following graph. 

```{r price_hist, echo=FALSE}
ggplot(data = ames_train, aes(log(price))) + geom_histogram(binwidth = 0.1) + scale_x_continuous(labels = dollar) + xlab("Price") + ylab("Observations")
# ggplot(data = ames_train, aes(log(price))) + geom_histogram() + scale_x_continuous(labels = dollar)
# ames_train %>% summarise(median(price))
```

All of the data points were examined for association with price. Several variables stood out as especially likely predictors of price. First, overall quality appears to be highly related to price. The following boxplot shows differences in price by overall quality.

```{r overall_qual, echo=FALSE}
ggplot(data = ames_train, aes(y = price, x = factor(Overall.Qual))) + geom_boxplot() + scale_y_continuous(labels = dollar) + xlab("Overall Quality") + ylab("Price")
```

Secondly, home squarefootage appears to be related to price, and the relationship appears more linear with a log transform on both price and area.

```{r area, echo=FALSE}
ggplot(data = ames_train, aes(y = log(price), x = log(area))) + geom_point() + scale_y_continuous(labels = dollar) + xlab("Square Feet (Log)") + ylab("Price (Log)")
```

Finally, neighborhood appears to be associated with price. Neighborhood can capture features not included in the dataset, including walkability to local amenities, local school quality, and other features. The following is a boxplot of housing price by neighborhood.

```{r neighborhood, echo=FALSE}
ggplot(data = ames_train, aes(Neighborhood, price)) + geom_boxplot() + scale_y_continuous(labels = dollar) + coord_flip() + ylab("Price")
```


* * *

## Part 2 - Development and assessment of an initial model, following a semi-guided process of analysis

### Section 2.1 An Initial Model
In building a model, it is often useful to start by creating a simple, intuitive initial model based on the results of the exploratory data analysis. (Note: The goal at this stage is **not** to identify the "best" possible model but rather to choose a reasonable and understandable starting point. Later you will expand and revise this model to create your final model.

Based on your EDA, select *at most* 10 predictor variables from “ames_train” and create a linear model for `price` (or a transformed version of price) using those variables. Provide the *R code* and the *summary output table* for your model, a *brief justification* for the variables you have chosen, and a *brief discussion* of the model results in context (focused on the variables that appear to be important predictors and how they relate to sales price).

* * *

Square footage and lot area appear to be related to price, which make sense as these values directly represent the amount of living space and overall space on a property. The presence of a basement provides additional space living and storage, and is likely to contribute to higher prices. Overall quality appears to be related to price, which makes sense, as this likely indicates how desireable the various features on a property might be to a prospective buyer. Bathrooms and bedrooms are typically saught after features, as are pools. The year of construction likely determines whether a building needs upgrades or improvements, which might affect the final price. Finally, some building types are more desireable than others, and this might have an influence on the final price. I factored all of these values into the initial model.

```{r fit_model}

initial.model <- lm(log(price) ~ 
                      log(area) + 
                      log(Lot.Area) + 
                      basement_dummy + 
                      Overall.Qual + 
                      bathrooms +
                      Bedroom.AbvGr +
                      Year.Built +
                      pool_dummy + 
                      Bldg.Type
                      ,data=ames_train_red)

summary(initial.model)

```

The model explains 87% of the variance in purchase price, according to the adjusted R^2 value. The pool variable seems to be an insignificant predictor variable, and building type is only marginally signficiant. These values will likely be excluded in future models.

* * *

### Section 2.2 Model Selection

Now either using `BAS` another stepwise selection procedure choose the "best" model you can, using your initial model as your starting point. Try at least two different model selection methods and compare their results. Do they both arrive at the same model or do they disagree? What do you think this means?

* * *

I used two model selection methods, backward selection by AIC and backward selection by BIC.

```{r model_select, echo=FALSE}

model.full <- lm(log(price) ~ ., data = ames_train_red)

model.AIC <- stepAIC(model.full, direction = "backward", k = 2, trace = FALSE)

# Summary AIC selected model
summary(model.AIC)

model.BIC <- stepAIC(model.full, direction = "backward", k = log(834), trace = FALSE)

# Summary BIC selected model
summary(model.BIC)

# AIC selected model RMSE
sqrt(mean(ames_train_red$price - exp(predict(model.AIC, ames_train_red))))

# BIC selected model RMSE
sqrt(mean(ames_train_red$price - exp(predict(model.BIC, ames_train_red))))

```

The AIC selected model includes the deck dummy variable, kitchen quality, land slope, condition 1, building type, number of above ground bedrooms, and several neighborhoods, roof styles and subclasses not selected by the BIC model. The BIC model had a lower adjusted R^2 value and RMSE, but is quite a bit simpler than the AIC selected model. The parismony of the BIC model is appealing, despite the lower fit metrics. The AIC model may be more overfit than the BIC model.

* * *

### Section 2.3 Initial Model Residuals
One way to assess the performance of a model is to examine the model's residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.

* * *

The residuals appear to be normally distributed and random. There are several outliers, which I'll examine more thoroughly in the validation section.

```{r model_resid}
# AIC selected model residuals
plot(model.AIC$residuals)

# BIC selected model residuals
plot(model.BIC$residuals)
```

* * *

### Section 2.4 Initial Model RMSE

You can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).

* * *

The root mean squared error is just over \$29 for the AIC selected model, and just over \$33 dollars for the BIC selected model.


```{r model_rmse}
# AIC selected model RMSE
sqrt(mean(ames_train_red$price - exp(predict(model.AIC, ames_train_red))))

# BIC selected model RMSE
sqrt(mean(ames_train_red$price - exp(predict(model.BIC, ames_train_red))))
```

* * *

### Section 2.5 Overfitting 

The process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set `ames_test`.

```{r loadtest, message=FALSE, echo=FALSE}
load("ames_test.Rdata")

ames_test %>% group_by(Sale.Condition) %>% summarise(records = n())
ames_test <- ames_test %>% filter(Sale.Condition == "Normal")
ames_test <- ames_test[!names(ames_test) == "Sale.Condition"]

ames_test$MS.SubClass <- as.character(ames_test$MS.SubClass) # change to character data type
ames_test$MS.SubClass[nchar(ames_test$MS.SubClass)==2] <- paste0("0",ames_test$MS.SubClass[nchar(ames_test$MS.SubClass)==2])

ames_test$Alley_vals <- as.character(ames_test$Alley)
ames_test$Alley_vals[is.na(ames_test$Alley_vals)] <- "No alley"

ames_test$basement_dummy <- TRUE
ames_test$basement_dummy[is.na(ames_test$Bsmt.Qual) | ames_test$Bsmt.Qual == ""] <- FALSE

ames_test$garage_dummy <- TRUE
ames_test$garage_dummy[is.na(ames_test$Garage.Type)] <- FALSE

ames_test$deck_dummy <- FALSE
ames_test$deck_dummy[ames_test$Wood.Deck.SF>0] <- TRUE

ames_test$porch_dummy <- FALSE
ames_test$porch_dummy[ames_test$Open.Porch.SF>0] <- TRUE
ames_test$porch_dummy[ames_test$Enclosed.Porch>0] <- TRUE
ames_test$porch_dummy[ames_test$X3Ssn.Porch>0] <- TRUE
ames_test$porch_dummy[ames_test$Screen.Porch>0] <- TRUE

ames_test$pool_dummy <- FALSE
ames_test$pool_dummy[ames_test$Pool.Area>0] <- TRUE

ames_test$fence_dummy <- TRUE
ames_test$fence_dummy[is.na(ames_test$Fence)] <- FALSE

ames_test <- ames_test %>% mutate(bathrooms = Full.Bath * 1 + Half.Bath * 0.5)

ames_test$area <- log(ames_test$area)
ames_test$Lot.Area <- log(ames_test$Lot.Area)

ames_test <- cbind(ames_test, model.matrix(~Neighborhood, ames_test))
ames_test <- cbind(ames_test,model.matrix(~MS.SubClass, ames_test))
ames_test <- cbind(ames_test,model.matrix(~House.Style, ames_test))
ames_test <- cbind(ames_test,model.matrix(~Roof.Style, ames_test))
ames_test <- cbind(ames_test,model.matrix(~Roof.Matl, ames_test))

ames_test <- ames_test[,!(names(ames_test) %in% drops)]

```

Use your model from above to generate predictions for the housing prices in the test data set.  Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data?  Why or why not? Briefly explain how you determined that (what steps or processes did you use)?

* * *

The RMSE values for the AIC and BIC models are both higher in the ames_test data than the training data. This suggests that the model is overfit, though the values are highter, the RMSE values are still less than $60, which indicates the predictions are fairly accurate.

```{r initmodel_test, echo=FALSE}
# AIC selected model test set prediction RMSE
sqrt(mean(ames_test$price - exp(predict(model.AIC, ames_test))))

# BIC selected model test set prediction RMSE
sqrt(mean(ames_test$price - exp(predict(model.BIC, ames_test))))
```

* * *

**Note to the learner:** If in real-life practice this out-of-sample analysis shows evidence that the training data fits your model a lot better than the test data, it is probably a good idea to go back and revise the model (usually by simplifying the model) to reduce this overfitting. For simplicity, we do not ask you to do this on the assignment, however.

## Part 3 Development of a Final Model

Now that you have developed an initial model to use as a baseline, create a final model with *at most* 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset and using any of the tools that we introduced in this specialization.  

Carefully document the process that you used to come up with your final model, so that you can answer the questions below.

### Section 3.1 Final Model

Provide the summary table for your model.

* * *

I compared a backwards selection by AIC with a forwards selection by AIC and and backwards and forwards selection by BIC. The forward selected models were the best fits to the training data. But the best fit to the test data was the backwards selected by AIC model. All four models were overfit.


```{r model_playground, echo=FALSE, message=FALSE}
model.AIC.forward <- stepAIC(model.full, direction = "forward", k = 2, trace = FALSE)



model.BIC.forward <- stepAIC(model.full, direction = "forward", k = log(834), trace = FALSE)


sqrt(mean(ames_train_red$price - exp(predict(model.AIC.forward, ames_train_red))))

sqrt(mean(ames_train_red$price - exp(predict(model.BIC.forward, ames_train_red))))


sqrt(mean(ames_test$price - exp(predict(model.AIC, ames_test))))
sqrt(mean(ames_test$price - exp(predict(model.AIC.forward, ames_test))))
sqrt(mean(ames_test$price - exp(predict(model.BIC, ames_test))))
sqrt(mean(ames_test$price - exp(predict(model.BIC.forward, ames_test))))



```

```{r final_model}
model.proposed <- lm(log(price) ~ area + Lot.Area + Land.Slope + Condition.1 + Bldg.Type + Overall.Qual + Overall.Cond + Year.Built + 
                       Year.Remod.Add + Exter.Qual + Central.Air + Bedroom.AbvGr + Kitchen.AbvGr + Kitchen.Qual + Functional + 
                       Fireplaces + basement_dummy + deck_dummy + NeighborhoodBlueste + NeighborhoodBrkSide + NeighborhoodCollgCr + 
                       NeighborhoodCrawfor + NeighborhoodEdwards + NeighborhoodGilbert + NeighborhoodGreens + NeighborhoodGrnHill + 
                       NeighborhoodMeadowV + NeighborhoodNoRidge + NeighborhoodNridgHt + NeighborhoodNWAmes + NeighborhoodSawyerW + 
                       NeighborhoodSomerst + NeighborhoodStoneBr + NeighborhoodVeenker + MS.SubClass030 + MS.SubClass040 + 
                       MS.SubClass050 + MS.SubClass060 + MS.SubClass080 + MS.SubClass085 + MS.SubClass120 + MS.SubClass190,
                     data = ames_train_red)

# Proposed model summary
summary(model.proposed)

# Proposed model training set RMSE
sqrt(mean(ames_train_red$price - exp(predict(model.proposed, ames_train_red))))
# Proposed model test set RMSE
sqrt(mean(ames_test$price - exp(predict(model.proposed, ames_test))))


ames_test$predicted <- exp(predict(model.proposed, ames_test))
ames_test$resids <- ames_test$price - ames_test$predicted
ames_test <- ames_test[,!(names(ames_test) == "(Intercept)")]
```


* * *

### Section 3.2 Transformation

* * *

As discussed in section 1, I transformed several variables, including price, area, and Lot.Area. The distribution of values for these variables were right-skewed, and when plotted against eachother looked non-linear. I performed a log transformation of all three variables before inputting into the model.

Additionally, I created some dummy variables for variables with incomeplete cases, where the absence of a value indicated that the feature was not present. This allowed me to use the precense (or lack thereof) of a feature in the model. I did this for pool, basement, fence, porch, and deck.

Further, I needed to do some transformations to change the data types of several variables in order to make them useful in the model. MS.SubClass was imported as a continuous value, though it should be treated as a discrete nominal value.

* * *

### Section 3.3 Variable Interaction

Did you decide to include any variable interactions? Why or why not? Explain in a few sentences.

* * *

I elected not to include any variable interaction in the model. A possible side-effect of modeling variable interactions is overfitting. Because the model is already overfit, I elected not to model any variable interactions.

* * *

### Section 3.4 Variable Selection

What method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.

* * *

I used a backward selection by AIC to select the variables to include in the model. This method involves creating a full model, with all variables considered for inclusion in the model, then removing variables one by one and comparing the AIC values for each simplified model. The model with the lowest AIC indicates the most parimonious model, and that process is repeated using that model as the new "full model." This process is repeated until removing a variable does not improve the AIC.



* * *

### Section 3.5 Model Testing

How did testing the model on out-of-sample data affect whether or how you changed your model? Explain in a few sentences.

* * *

I used RMSE to compare the results from the model chosen by backward selection by AIC with using three other methods: forward selection by AIC, backward selection by BIC, and forward selection by AIC. The backward selection by AIC had the best RMSE score for the test dataset. Though the RMSE results from all models indicate they are overfit, the difference is roughly $20, and that seems close enough for me.

* * *

## Part 4 Final Model Assessment

### Section 4.1 Final Model Residual

For your final model, create and briefly interpret an informative plot of the residuals.

* * *
The residuals appear to be randomly distributed and nearly normal, which is a good sign for the model.

```{r final_resids, echo=FALSE}
plot(model.proposed$residuals)
```


* * *

### Section 4.2 Final Model RMSE

For your final model, calculate and briefly comment on the RMSE.

* * *

The model's RMSE for the training data is \$34.86644, and for the test set is \$52.582. Though the model appears to be overfit, as the RMSE for the test set was appreciably different than the training set, the RMSE is fairly low relative to the median price of a home in the dataset.

* * *

### Section 4.3 Final Model Evaluation

What are some strengths and weaknesses of your model?

* * *

The model seems to underestimate the prices of homes in certain neighborhoods more than other neighborhoods. Specifically, homes in North Ridge, Northridge Heights, and Landmark seemed to be the most underestimated. I'd have included neighborhood in the model, but not all neighborhoods are represented in the training data.

* * *

### Section 4.4 Final Model Validation

Testing your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice. 

You will use the “ames_validation” dataset to do some additional assessment of your final model. Discuss your findings, be sure to mention:
* What is the RMSE of your final model when applied to the validation data?  
* How does this value compare to that of the training data and/or testing data?
* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?  
* From this result, does your final model properly reflect uncertainty?

```{r loadvalidation, message = FALSE}
load("ames_validation.Rdata")
ames_validation %>% group_by(Sale.Condition) %>% summarise(records = n())
ames_validation <- ames_validation %>% filter(Sale.Condition == "Normal")
ames_validation <- ames_validation[!names(ames_validation) == "Sale.Condition"]

ames_validation$MS.SubClass <- as.character(ames_validation$MS.SubClass) # change to character data type
ames_validation$MS.SubClass[nchar(ames_validation$MS.SubClass)==2] <- paste0("0",ames_validation$MS.SubClass[nchar(ames_validation$MS.SubClass)==2])

ames_validation$Alley_vals <- as.character(ames_validation$Alley)
ames_validation$Alley_vals[is.na(ames_validation$Alley_vals)] <- "No alley"

ames_validation$basement_dummy <- TRUE
ames_validation$basement_dummy[is.na(ames_validation$Bsmt.Qual) | ames_validation$Bsmt.Qual == ""] <- FALSE

ames_validation$garage_dummy <- TRUE
ames_validation$garage_dummy[is.na(ames_validation$Garage.Type)] <- FALSE

ames_validation$deck_dummy <- FALSE
ames_validation$deck_dummy[ames_validation$Wood.Deck.SF>0] <- TRUE

ames_validation$porch_dummy <- FALSE
ames_validation$porch_dummy[ames_validation$Open.Porch.SF>0] <- TRUE
ames_validation$porch_dummy[ames_validation$Enclosed.Porch>0] <- TRUE
ames_validation$porch_dummy[ames_validation$X3Ssn.Porch>0] <- TRUE
ames_validation$porch_dummy[ames_validation$Screen.Porch>0] <- TRUE

ames_validation$pool_dummy <- FALSE
ames_validation$pool_dummy[ames_validation$Pool.Area>0] <- TRUE

ames_validation$fence_dummy <- TRUE
ames_validation$fence_dummy[is.na(ames_validation$Fence)] <- FALSE

ames_validation <- ames_validation %>% mutate(bathrooms = Full.Bath * 1 + Half.Bath * 0.5)

ames_validation$area <- log(ames_validation$area)
ames_validation$Lot.Area <- log(ames_validation$Lot.Area)

ames_validation <- cbind(ames_validation, model.matrix(~Neighborhood, ames_validation))
ames_validation <- cbind(ames_validation,model.matrix(~MS.SubClass, ames_validation))
ames_validation <- cbind(ames_validation,model.matrix(~House.Style, ames_validation))
ames_validation <- cbind(ames_validation,model.matrix(~Roof.Style, ames_validation))
ames_validation <- cbind(ames_validation,model.matrix(~Roof.Matl, ames_validation))

ames_validation <- ames_validation[,!(names(ames_validation) %in% drops)]

```

* * *

The RMSE value for the validation data is just over $43, which is less than the RMSE for the test data, but more than the training data. Approximately 40% of the validation dataset falls within the 95% confidence interval, which suggests the model does not fully account for uncertainty in the market. 

The graph of residuals shows some apparent outliers on the high side, where the model is underpredicting the actual sale price for a number of homes. Given the goal of identifying undervalued homes, the risk aversion is on overestimating the value of homes, making this error acceptable, in my view.

```{r model_validate}
sqrt(mean(ames_validation$price - exp(predict(model.proposed, ames_validation))))
ames_validation <- cbind(ames_validation, predict(model.proposed, ames_validation, interval="confidence"))
ames_validation$fit <- exp(ames_validation$fit)
ames_validation$lwr <- exp(ames_validation$lwr)
ames_validation$upr <- exp(ames_validation$upr)
ames_validation$confidence <- FALSE
ames_validation$confidence[ames_validation$price <= ames_validation$upr & ames_validation$price >= ames_validation$lwr] <- TRUE
length(which(ames_validation$confidence==TRUE)) / length(ames_validation$confidence)
plot(ames_validation$price - ames_validation$fit)
ames_validation$resids <- ames_validation$price - ames_validation$fit
ames_validation <- ames_validation[,!(names(ames_validation) %in% c("(Intercept)", "lwr", "upr", "Heating.QC"))]
```

Several townhomes were underestimated by the model, which suggests some features to capture the value of townhomes would benefit future iterations of the model.

* * *

## Part 5 Conclusion

Provide a brief summary of your results, and a brief discussion of what you have learned about the data and your model. 

* * *

The model does fairly well, but struggles with predicting the highest value properties. The validation RMSE was 40.38, and roughly 38% of the true home prices fell within the 95% confidence interval. The model explains roughly 93% of the difference in home price. Overall condition, neighborhood, and home and lot area are significant predictors of home price. Surprisingly, the presence of a pool is not a signficiant predictor of home value.

* * *
